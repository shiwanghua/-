% !TeX encoding = GBK
\documentclass[journal, a4paper]{IEEEtran}
% some very useful LaTeX packages include:

%\usepackage{cite}      % Written by Donald Arseneau
                        % V1.6 and later of IEEEtran pre-defines the format
                        % of the cite.sty package \cite{} output to follow
                        % that of IEEE. Loading the cite package will
                        % result in citation numbers being automatically
                        % sorted and properly "ranged". i.e.,
                        % [1], [9], [2], [7], [5], [6]
                        % (without using cite.sty)
                        % will become:
                        % [1], [2], [5]--[7], [9] (using cite.sty)
                        % cite.sty's \cite will automatically add leading
                        % space, if needed. Use cite.sty's noadjust option
                        % (cite.sty V3.8 and later) if you want to turn this
                        % off. cite.sty is already installed on most LaTeX
                        % systems. The latest version can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/cite/

\usepackage{graphicx}   % Written by David Carlisle and Sebastian Rahtz
                        % Required if you want graphics, photos, etc.
                        % graphicx.sty is already installed on most LaTeX
                        % systems. The latest version and documentation can
                        % be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/required/graphics/
                        % Another good source of documentation is "Using
                        % Imported Graphics in LaTeX2e" by Keith Reckdahl
                        % which can be found as esplatex.ps and epslatex.pdf
                        % at: http://www.ctan.org/tex-archive/info/

%\usepackage{psfrag}    % Written by Craig Barratt, Michael C. Grant,
                        % and David Carlisle
                        % This package allows you to substitute LaTeX
                        % commands for text in imported EPS graphic files.
                        % In this way, LaTeX symbols can be placed into
                        % graphics that have been generated by other
                        % applications. You must use latex->dvips->ps2pdf
                        % workflow (not direct pdf output from pdflatex) if
                        % you wish to use this capability because it works
                        % via some PostScript tricks. Alternatively, the====+++
                        % graphics could be processed as separate files via
                        % psfrag and dvips, then converted to PDF for
                        % inclusion in the main file which uses pdflatex.
                        % Docs are in "The PSfrag System" by Michael C. Grant
                        % and David Carlisle. There is also some information
                        % about using psfrag in "Using Imported Graphics in
                        % LaTeX2e" by Keith Reckdahl which documents the
                        % graphicx package (see above). The psfrag package
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/psfrag/

%\usepackage{subfigure} % Written by Steven Douglas Cochran
                        % This package makes it easy to put subfigures
                        % in your figures. i.e., "figure 1a and 1b"
                        % Docs are in "Using Imported Graphics in LaTeX2e"
                        % by Keith Reckdahl which also documents the graphicx
                        % package (see above). subfigure.sty is already
                        % installed on most LaTeX systems. The latest version
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/subfigure/
%\usepackage[hidelinks]{hyperref}
\usepackage{hyperref}

%\usepackage{url}        % Written by Donald Arseneau
                        % Provides better support for handling and breaking
                        % URLs. url.sty is already installed on most LaTeX
                        % systems. The latest version can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/other/misc/
                        % Read the url.sty source comments for usage information.

%\usepackage{stfloats}  % Written by Sigitas Tolusis
                        % Gives LaTeX2e the ability to do double column
                        % floats at the bottom of the page as well as the top.
                        % (e.g., "\begin{figure*}[!b]" is not normally
                        % possible in LaTeX2e). This is an invasive package
                        % which rewrites many portions of the LaTeX2e output
                        % routines. It may not work with other packages that
                        % modify the LaTeX2e output routine and/or with other
                        % versions of LaTeX. The latest version and
                        % documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/contrib/supported/sttools/
                        % Documentation is contained in the stfloats.sty
                        % comments as well as in the presfull.pdf file.
                        % Do not use the stfloats baselinefloat ability as
                        % IEEE does not allow \baselineskip to stretch.
                        % Authors submitting work to the IEEE should note
                        % that IEEE rarely uses double column equations and
                        % that authors should try to avoid such use.
                        % Do not be tempted to use the cuted.sty or
                        % midfloat.sty package (by the same author) as IEEE
                        % does not format its papers in such ways.

\usepackage{amsmath}    % From the American Mathematical Society
                        % A popular package that provides many helpful commands
                        % for dealing with mathematics. Note that the AMSmath
                        % package sets \interdisplaylinepenalty to 10000 thus
                        % preventing page breaks from occurring within multiline
                        % equations. Use:
%\interdisplaylinepenalty=2500
                        % after loading amsmath to restore such page breaks
                        % as IEEEtran.cls normally does. amsmath.sty is already
                        % installed on most LaTeX systems. The latest version
                        % and documentation can be obtained at:
                        % http://www.ctan.org/tex-archive/macros/latex/required/amslatex/math/
\usepackage{listings}
\usepackage{xcolor}  
                        
\hypersetup{
	colorlinks=true,
	linkcolor=black,
	urlcolor=red,
	citecolor=red
}


% Other popular packages for formatting tables and equations include:

%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty which improves the
% LaTeX2e array and tabular environments to provide better appearances and
% additional user controls. array.sty is already installed on most systems.
% The latest version and documentation can be obtained at:
% http://www.ctan.org/tex-archive/macros/latex/required/tools/

% V1.6 of IEEEtran contains the IEEEeqnarray family of commands that can
% be used to generate multiline equations as well as matrices, tables, etc.

% Also of notable interest:
% Scott Pakin's eqparbox package for creating (automatically sized) equal
% width boxes. Available:
% http://www.ctan.org/tex-archive/macros/latex/contrib/supported/eqparbox/

% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.


% Your document starts here!
\begin{document}
\begin{titlepage}

\newcommand{\HRule}{\rule{\linewidth}{0.5mm}} % Defines a new command for the horizontal lines, change thickness here

\center % Center everything on the page
 %----------------------------------------------------------------------------------------
%	LOGO SECTION
%----------------------------------------------------------------------------------------

~\\[1cm]
\includegraphics{SCUT.png}\\[2cm] % Include a department/university logo - this will require the graphicx package

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\HRule \\[1cm]
{ \huge \bfseries The Experiment Report of \textit{Machine Learning} }\\[0.6cm] % Title of your document
\HRule \\[2cm]
%----------------------------------------------------------------------------------------
%	HEADING SECTIONS
%----------------------------------------------------------------------------------------


\textsc{\LARGE \textbf{School:} School of Software Engineering}\\[1cm]
\textsc{\LARGE \textbf{Subject:} Software Engineering}\\[2cm] 

 
%----------------------------------------------------------------------------------------
%	AUTHOR SECTION
%----------------------------------------------------------------------------------------

\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Author:}\\
Wanghua Shi   \\ % Your name 
Wenjun Liang   \\
Weiwen Hu 
\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Supervisor:} \\
Mingkui Tan or Qingyao Wu % Supervisor's Name
\end{flushright}
\end{minipage}\\[2cm]
~
\begin{minipage}{0.4\textwidth}
\begin{flushleft} \large
\emph{Student ID:}\\
201630676843 \\
201630664963  \\
201630676713


\end{flushleft}
\end{minipage}
~
\begin{minipage}{0.4\textwidth}
\begin{flushright} \large
\emph{Grade:} \\
Undergraduate 
\end{flushright}
\end{minipage}\\[2cm]

% If you don't want a supervisor, uncomment the two lines below and remove the section above
%\Large \emph{Author:}\\
%John \textsc{Smith}\\[3cm] % Your name

%----------------------------------------------------------------------------------------
%	DATE SECTION
%----------------------------------------------------------------------------------------

{\large \today}\\[2cm] % Date, change the \today to a set date if you want to be precise

 
%----------------------------------------------------------------------------------------

\vfill % Fill the rest of the page with whitespace

\end{titlepage}

% Define document title and author
	\title{Face Detection Based on Neural Network}
	\maketitle

% Write abstract here
\begin{abstract}
Based on reading and understanding the principles of the given paper <<Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks>>, we run the given codes with well trained network model on sixty-four test pictures. Finally, we got 64 pictures which are well marked with a rectangular box and five facial landmarks positions.
\end{abstract}

% Each section begins with a \section{title} command
\section{Introduction}
Face detection and alignment are essential to many face applications, such as face recognition and facial expression analysis. However, the large visual variations of faces, such as occlusions, large pose variations and extreme lightings, impose great challenges for these tasks in real world applications. Besides, most of previous face detection and face alignment methods ignore the inherent correlation between these two tasks. Though several existing works attempt to jointly solve them, there are still limitations in these works. On the other hand, mining hard samples in training is critical to strengthen the power of detector. Nevertheless, traditional hard sample mining usually performs in an offline manner, which significantly increases the manual operations. To solving these tasks discussed above, we learned and adopted the unified cascaded CNN proposed in the paper \cite{1}.
%\subsection{Problems}
%The main problems to be solved within my reserarch are shown as below:
%	\begin{quote}
%	\begin{enumerate} 
%	\item Extract NPD features correctly
%	\item Achieve Adaboost by aid of DecisionTreeClassifier
%	\item Adjust model, update parameters and improve the accuracy of the prediction
%	\end{enumerate} 
%    \end{quote}		
%		
%\subsection{Motivations}
%	The motivations of this experiment are shown below:
%	\begin{quote}
%	\begin{enumerate} 
%	\item Understand Adaboost algorithm further
%	\item Get familiar with the basic method of face detection
%	\item Learn to use Adaboost to solve the face detection problem, and combine the theory with the actual project
%	\item Experience an complete process of machine learning
%	\end{enumerate} 
%    \end{quote}	
%\subsection{Expectation}
%	My expectation of this experiment have three points:
%	\begin{quote}
%		\begin{enumerate}
%			\item Extract the features and save them to the local file
%			\item Use adaboost algorithm and get an accuracy rate more than 0.9 on the given data set.
%			\item Use OpenCV's build-in method based on Haar feature to achieve face detection 
%		\end{enumerate}
%	\end{quote}
\par

% Main Part
\section{Methods and Theory }\label{2}
\subsection{Overall Framework}
The proposed Cascaded Convolutional Networks consist of three stages. A brief description is given below.
\begin{enumerate}
	\item Produce candidate windows quickly through a shallow CNN: Exploit a fully convolutional network, called Proposal Network (P-Net), to obtain the candidate facial windows and their bounding box regression vectors. Employ non-maximum suppression (NMS) to merge highly overlapped candidates. 
	\item Refines the windows by rejecting a large number of non-faces windows through a more complex CNN called Refine Network (R-Net). Performs calibration with bounding box regression and conducts NMS again. 
	\item Uses a more powerful CNN to refine the result again and output five facial landmarks' positions.
\end{enumerate}
\subsection{CNN Architectures Improvements}
By analyzing the limitations of multiple CNNs in \cite{2}, reduce the number of filters and change the 5$\times$5 filter to 3$\times$3 filter to reduce the computing while increasing the depth to get better performance.  After the convolution and fully connection layers (except output layers),  apply PReLU \cite{3} as non-linearity activation function.  

\subsection{Training Model Selection}
\begin{itemize}
	\item Train CNN Detectors \par
    Three tasks of training CNN detectors are leveraged: face/non-face classification, bounding box regression, and facial landmark localization. Details are summarized below.
\begin{enumerate}
	\item Face Classification: Use the cross-entropy loss:
	\begin{equation}
 \mathop L\nolimits_i^{\det }  =  - (y_i^{\det }\log ({p_i}) + (1 - y_i^{\det })(1 - \log ({p_i})))
	\end{equation}
	where ${p_i}$ is the probability produced by the network that indicates sample $x_i$ being a face. The notation $\mathop y\nolimits_i^{\det }  \in \left\{ {0,1} \right\}$ denotes the ground-truth label. 
	\item Bounding box regression: For each candidate window, predict the offset between it and the nearest ground truth, employ the Euclidean loss for each sample $x_i$.
	\item Facial landmark localization: Still use the Euclidean loss and minimize it. Since there are five facial landmarks, $ y_i^{landmark} \in {{\bf{R}}^{10}}$.
\end{enumerate}    
\item Multi-Source Training \par
 Use a sample type indicator to handle different types of training images in the learning process. The overall learning target can be formulated as:
\begin{equation}    
\min \sum\limits_{i = 1}^N {\sum\limits_{}^{} {_{j \in \{ \det,box,landmark\} }{\alpha _j}\beta _i^jL_i^j} } 
\end{equation} 
where the N is the number of training samples and $\alpha_j$ denotes on the task importance. $\beta_i^j \in \left\{{0,1} \right\} $ is the sample type indicator. 
\par
\item Online Hard Sample Mining \par
 \cite{1} conducts online hard sample mining which is adaptive to the training process. In each mini-batch, there are two steps:
 \begin{enumerate}
 	\item Sort the losses computed in the forward propagation from all samples and select the 70\% of them as hard samples.
 	\item Only compute the gradients from these hard samples in the backward propagation. 
 \end{enumerate}  
\end{itemize}
%In this section, you are asked to give a complete introduction to the experiment. For instance, the chosen methods, the related theories, the related equations(loss function), the derivation process(taking the gradient) and so on.

\section{Experiments}
\subsection{Dataset}
\begin{enumerate}
	\item Use \href{http://mmlab.ie.cuhk.edu.hk/projects/WIDERFace/}{WiderFace} for face classification and face bounding box regression when training PNet, RNet and ONet. 
	\item Use \href{http://mmlab.ie.cuhk.edu.hk/archive/CNN_FacePoint.htm}{Training Data Set} for face feature point regression. 
\end{enumerate}

\subsection{Environment for Experiment} 
\begin{itemize}
	\item \href{https://www.anaconda.com/download/}{Anaconda3}
	\item \href{https://pytorch.org/}{pytorch 0.4.1}
	\item \href{https://pypi.org/project/opencv-python/}{opencv-python}
	\item \href{https://tensorflow.google.cn/install/pip}{tensorflow}(only for python 3.4,3.5,3.6)
\end{itemize}

\subsection{Implementation}
%All detailed implementation in your experiment: initialization, process, results, all kinds of parameters. In a word, describe clearly What you do and how you do.\par
%\subparagraph{Face Classification}
\begin{itemize}
	\item Read the paper about MTCNN \cite{1}
%\subsubsection{Face Classification}
    We have printed the five-page paper, read it carefully and summarized it in the part \ref{2}. 
   \item Run the given codes 
   \begin{enumerate}
   	\item Get the complete codes in \href{https://github.com/wujiaju/mtcnn_pytorch}{MTCNN\_pytorch}. 
   	\item Install the environment (Details Omitted).
   	\item Test the given models: Run the file ``test\_image.py"(Use the command below), and examined the results in the path ``.../mtcnn\_pytorch/data/you\_result/". 
%   	\par
%\lstset{language=commandshell}
%
\begin{lstlisting}[mathescape, includerangemarker=false,backgroundcolor=\color{yellow!15},  frame=single,numbers=left,firstnumber=1 ] 
cd mtcnn_pytorch/
python test_image.py
 \end{lstlisting}
\end{enumerate}
\end{itemize} \par
\indent We have found all the faces of test images have been framed in a rectangle and five facial landmarks positions of faces (left eye, right eye, nose, left mouth corner and right mouth corner) have been marked with red points. Besides, the generating speed of trained model is fast.
	% You can reference tables and figure by using the \ref{label} command. Each table and figure needs to have a UNIQUE label.

	% This is how you define a table: the [!hbt] means that LaTeX is forced (by the !) to place the table exactly here (by h), or if that doesnt work because of a pagebreak or so, it tries to place the table to the bottom of the page (by b) or the top (by t).

	% If you have questions about how to write mathematical formulas in LaTeX, please read a LaTeX book or the 'Not So Short Introduction to LaTeX': tobi.oetiker.ch/lshort/lshort.pdf

	% This is how you include a eps figure in your document. LaTeX only accepts EPS or TIFF files.

\section{Conclusion}
	In this experiment, we have adopted a multi-task CNNs based framework for joint face detection and alignment. We made our efforts to understand the basic theory of face detection using neural network, understand the processes of MTCNN and use it in practice. Eventually, we have a basic understanding of CNNs, online hard samples mining strategy and joint face alignment learning.

\begin{thebibliography}{99}

	\bibitem{1} Kaipeng Zhang. Joint Face Detection and Alignment using   Multi-task Cascaded Convolutional Networks, 2016.
	\bibitem{2}  H. Li, Z. Lin, X. Shen, J. Brandt, and G. Hua, ``A convolutional neural network cascade for face detection" in IEEE Conference on Computer Vision and Pattern Recognition, 2015, pp. 5325-5334.
	\bibitem{3} K. He, X. Zhang, S. Ren, J. Sun, ``Delving deep into rectifiers: Surpassing human-level performance on imagenet classification" in IEEE International Conference on Computer Vision, 2015, pp. 1026-1034. 
%	 \url{http://www.latexstudio.net/}
\end{thebibliography}

\begin{appendices}
	\section{Task Division Table in This Experiment}
	\begin{center}
		\begin{tabular}{c|l}
			\hline
		  Group Member & Main Task \\
		     \\
			\hline
		    Wanghua Shi & Read paper, run codes and \\
		    & write experiment report \\
		    \hline
		    Wenjun Liang &  Read paper and understand model   \\  \hline
			Weiwen Hu &   Read codes and     \\
			&  understand the algorithm \\
			\hline
		\end{tabular}
	\end{center}
\end{appendices}
% Your document ends here!
\end{document}